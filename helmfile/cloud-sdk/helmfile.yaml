repositories:
- name: stable
  url: https://charts.helm.sh/stable
- name: kiwigrid 
  url: https://kiwigrid.github.io
- name: nginx
  url: https://kubernetes.github.io/ingress-nginx
- name: eks
  url: https://aws.github.io/eks-charts

environments:
  azure:
    values:
    - monitoring:
        namespace: monitoring
    - domain: tip.4c74356b41.com
    - storageClass: default
    - autoscaler:
        enabled: true
    - ingress:
        enabled: true
    - elastic:
        enabled: true
    - kibana:
        enabled: true
    - prometheus:
        enabled: true
    - external-dns:
        enabled: true

  amazon-cicd:
    values:
    - eks:
        clusterName: tip-wlan-main
        region: us-east-2
        accountID: 289708231103
        hostedZoneId: cicd
        certificateARN: arn:aws:acm:us-east-2:289708231103:certificate/bfa89c7a-5b64-4a8a-bcfe-ffec655b5285
    - monitoring:
        namespace: monitoring
    - domain: lab.wlan.tip.build
    - storageClass: gp2
    - autoscaler:
        enabled: true
    - ingress:
        enabled: false
    - elastic:
        enabled: true
    - kibana:
        enabled: true
    - prometheus:
        enabled: true
    - external-dns:
        enabled: true
    - alb-ingress:
        enabled: true
    - node-termination-handler:
        enabled: true

  amazon-qa:
    values:
    - eks:
        clusterName: tip-wlan-qa
        region: us-east-2
        accountID: 289708231103
        hostedZoneId: Z02683893QSUHV3RLVNVB
        certificateARN: arn:aws:acm:us-east-2:289708231103:certificate/a6dfde32-93bb-4a14-949f-dab16588c539
    - monitoring:
        namespace: monitoring
    - domain: lab.wlan.tip.build
    - storageClass: gp2
    - autoscaler:
        enabled: true
    - ingress:
        enabled: false
    - elastic:
        enabled: true
    - kibana:
        enabled: true
    - prometheus:
        enabled: true
    - external-dns:
        enabled: true
    - alb-ingress:
        enabled: true
    - node-termination-handler:
        enabled: true

helmDefaults:
  force: false
  timeout: 300
  # dont seem to work
  # wait: false
  # recreatePods: true
  # verify: true

templates:
  default: &default
    namespace: kube-system
    missingFileHandler: Warn
  cluster-autoscaler: &cluster-autoscaler
    values:
    - envs/common/cluster-autoscaler.yaml.gotmpl
  external-dns: &external-dns
    values:
    - envs/common/external-dns.yaml.gotmpl

# core setup
releases:
- name: cluster-autoscaler
  condition: autoscaler.enabled
  <<: *default 
  <<: *cluster-autoscaler
  chart: stable/cluster-autoscaler
  version: 7.3.2
  labels:
    role: setup
    group: system
    app: autoscaler
- name: external-dns
  condition: external-dns.enabled
  <<: *default
  <<: *external-dns
  chart: stable/external-dns
  version: 2.20.4
  labels:
    role: setup
    group: system
    app: external-dns
- name: nginx-ingress
  condition: ingress.enabled
  <<: *default
  chart: nginx/ingress-nginx
  version: 3.4.0
  labels:
    role: setup
    group: system
    app: ingress
  values:
  - controller:
      ingressClass: nginx-sso
      service:
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-ssl-cert: {{ .Environment.Values.eks.certificateARN }}
          service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https
          service.beta.kubernetes.io/aws-load-balancer-type: elb
          service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
        targetPorts:
          http: http
          https: http
      publishService:
        enabled: true
      metrics:
        enabled: true
        serviceMonitor:
          enabled: true
          additionalLabels:
            release: prometheus-operator
  - defaultBackend:
      enabled: true
      image:
        repository: 4c74356b41/custom-backend
        tag: latest

# monitoring
- name: prometheus-operator
  condition: prometheus.enabled
  namespace: {{ .Environment.Values.monitoring.namespace }}
  chart: stable/prometheus-operator
  labels:
    role: setup
    group: monitoring
    app: prometheus-operator
  values:
  - prometheusOperator:
      manageCrds: true
      createCustomResource: false
  - prometheus:
      enabled: true
      prometheusSpec:
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: gp2
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
      ingress:
        enabled: true
        annotations:
          nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
          kubernetes.io/ingress.class: nginx-sso
        hosts:
        - prometheus.{{ .Environment.Values.domain }}
  - grafana:
      grafana.ini:
        users:
          viewers_can_edit: true
        auth:
          disable_login_form: true
          disable_signout_menu: true
        auth.anonymous:
          enabled: true
          org_role: Viewer
      testFramework:
        enabled: false
      ingress:
        enabled: true
        annotations:
          nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
          kubernetes.io/ingress.class: nginx-sso
        hosts:
        - grafana.{{ .Environment.Values.domain }}
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: 'default'
            orgId: 1
            folder: imported
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          mosquitto:
            gnetId: 11542
            revision: 1
            datasource: Prometheus
          nginx-ingress:
            gnetId: 9614
            revision: 1
            datasource: Prometheus
          postgres:
            gnetId: 6742
            revision: 1
            datasource: Prometheus
          # still need to finalize the dashboards below but they are working partially
          cassandra:
            # gnetId: 5408
            gnetId: 6258
            revision: 3
            datasource: Prometheus
          kafka:
            gnetId: 7589
            revision: 5
            # gnetId: 10555
            # revision: 1
            datasource: Prometheus
          jvm-metrics-per-pod:
            datasource: Prometheus
            json: |
            {{- readFile "grafana-dashboards/jvm-metrics-per-pod.json" | nindent 14 }}
          cloud-controller-perf:
            datasource: Prometheus
            json: |
            {{- readFile "grafana-dashboards/cloud-controller-perf.json" | nindent 14 }}
          pod-events:
            datasource: Prometheus
            json: |
            {{- readFile "grafana-dashboards/pod-events.json" | nindent 14 }}

      datasources:
       datasources.yaml:
        apiVersion: 1
        datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-operated:9090
          access: proxy
          isDefault: false
- name: prometheus-operator-helper
  condition: prometheus.enabled
  namespace: {{ .Environment.Values.monitoring.namespace }}
  chart: charts/standalone-monitoring
  labels:
    role: setup
    group: monitoring
    app: prometheus-operator
    sub: helper
  values:
  - monitoring:
      namespace: {{ .Environment.Values.monitoring.namespace }}
      domain: {{ .Environment.Values.domain }}
    proxy:
      namespace: kube-system
- name: prometheus-operator-ingress-auth
  condition: prometheus.enabled
  namespace: kube-system
  chart: charts/sso
  labels:
    role: setup
    group: monitoring
    app: prometheus-operator
    sub: oAuth
  values:
  - charts/sso/values.local.yaml # temporary solution to workaround secrets in plain text in the repo
  - monitoring:
      namespace: {{ .Environment.Values.monitoring.namespace }}
- name: fluentd
  condition: elastic.enabled
  namespace: {{ .Environment.Values.monitoring.namespace }}
  chart: kiwigrid/fluentd-elasticsearch
  labels:
    role: setup
    group: monitoring
    app: fluentd
  values:
  - elasticsearch:
      serviceAccount:
        create: true
      awsSigningSidecar:
        enabled: false
      hosts:
      - elasticsearch-client.{{ .Environment.Values.monitoring.namespace }}.svc.cluster.local 
- name: elasticsearch
  condition: elastic.enabled
  namespace: {{ .Environment.Values.monitoring.namespace }}
  chart: stable/elasticsearch
  labels:
    role: setup
    group: monitoring
    app: elasticsearch
- name: kibana
  condition: kibana.enabled
  namespace: {{ .Environment.Values.monitoring.namespace }}
  chart: stable/kibana
  labels:
    role: setup
    group: monitoring
    app: kibana
  values:
  - testFramework:
      enabled: false
  - image:
      tag: "6.8.6"
  - files:
      kibana.yml:
        elasticsearch.hosts: http://elasticsearch-client.{{ .Environment.Values.monitoring.namespace }}.svc.cluster.local:9200
  - ingress:
      enabled: true
      annotations:
        nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
        nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
        kubernetes.io/ingress.class: nginx-sso
      hosts:
      - kibana.{{ .Environment.Values.domain }}
- name: aws-load-balancer-controller
  <<: *default
  condition: alb-ingress.enabled
  chart: eks/aws-load-balancer-controller
  version: 1.0.5
  values:
  - serviceAccount:
      annotations:
        eks.amazonaws.com/role-arn: arn:aws:iam::{{ .Values.eks.accountID }}:role/{{ .Values.eks.clusterName }}-alb-ingress
    clusterName: {{ .Values.eks.clusterName }}
    enableShield: false
    enableWaf: false
    enableWafv2: false
    logLevel: debug
- name: aws-node-termination-handler
  <<: *default
  condition: node-termination-handler.enabled
  chart: eks/aws-node-termination-handler
  version: 0.13.2
  labels:
    role: setup
    group: system
    app: node-termination-handler
  values:
    - deleteLocalData: true
    - podTerminationGracePeriod: -1 # use values defined in Pod
